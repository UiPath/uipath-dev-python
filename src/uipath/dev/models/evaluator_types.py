"""Evaluator type definitions and schemas."""

from typing import Any


EVALUATOR_TYPES: dict[str, dict[str, Any]] = {
    "uipath-contains": {
        "name": "Contains Evaluator",
        "description": "Checks if the response text includes the expected search text.",
        "category": "output",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "ContainsEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "targetOutputKey",
                "label": "Target Output Key",
                "type": "string",
                "default": "*",
                "required": False,
                "description": "Key to extract output from agent execution",
            },
            {
                "name": "ignoreCase",
                "label": "Ignore Case",
                "type": "boolean",
                "default": False,
                "required": False,
            },
            {
                "name": "negated",
                "label": "Negated",
                "type": "boolean",
                "default": False,
                "required": False,
                "description": "If true, checks that text does NOT contain the search text",
            },
        ],
        "criteria_fields": [
            {
                "name": "searchText",
                "label": "Search Text",
                "type": "string",
                "required": True,
            },
        ],
    },
    "uipath-exact-match": {
        "name": "Exact Match Evaluator",
        "description": "Checks if the response text exactly matches the expected value.",
        "category": "output",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "ExactMatchEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "targetOutputKey",
                "label": "Target Output Key",
                "type": "string",
                "default": "*",
                "required": False,
                "description": "Key to extract output from agent execution",
            },
            {
                "name": "ignoreCase",
                "label": "Ignore Case",
                "type": "boolean",
                "default": False,
                "required": False,
            },
            {
                "name": "negated",
                "label": "Negated",
                "type": "boolean",
                "default": False,
                "required": False,
            },
        ],
        "criteria_fields": [
            {
                "name": "expectedOutput",
                "label": "Expected Output",
                "type": "json",
                "required": True,
                "description": "The expected output (string or object)",
            },
        ],
    },
    "uipath-json-similarity": {
        "name": "JSON Similarity Evaluator",
        "description": "Checks if the response JSON is similar to the expected JSON structure.",
        "category": "output",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "JsonSimilarityEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "targetOutputKey",
                "label": "Target Output Key",
                "type": "string",
                "default": "*",
                "required": False,
                "description": "Key to extract output from agent execution",
            },
        ],
        "criteria_fields": [
            {
                "name": "expectedOutput",
                "label": "Expected Output",
                "type": "json",
                "required": True,
                "description": "The expected JSON output",
            },
        ],
    },
    "uipath-llm-judge-output-semantic-similarity": {
        "name": "LLM Judge Output Evaluator",
        "description": "Uses an LLM to judge semantic similarity between expected and actual output.",
        "category": "output",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "LLMJudgeOutputEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "targetOutputKey",
                "label": "Target Output Key",
                "type": "string",
                "default": "*",
                "required": False,
            },
            {
                "name": "model",
                "label": "Model",
                "type": "string",
                "default": "",
                "required": False,
                "description": "LLM model to use for judging",
            },
            {
                "name": "prompt",
                "label": "Prompt",
                "type": "textarea",
                "default": "As an expert evaluator, analyze the semantic similarity of these JSON contents to determine a score from 0-100. Focus on comparing the meaning and contextual equivalence of corresponding fields, accounting for alternative valid expressions, synonyms, and reasonable variations in language while maintaining high standards for accuracy and completeness. Provide your score with a justification, explaining briefly and concisely why you gave that score.\n----\nExpectedOutput:\n{{ExpectedOutput}}\n----\nActualOutput:\n{{ActualOutput}}",
                "required": False,
            },
            {
                "name": "temperature",
                "label": "Temperature",
                "type": "number",
                "default": 0.0,
                "required": False,
            },
            {
                "name": "maxTokens",
                "label": "Max Tokens",
                "type": "integer",
                "default": None,
                "required": False,
            },
        ],
        "criteria_fields": [
            {
                "name": "expectedOutput",
                "label": "Expected Output",
                "type": "json",
                "required": True,
            },
        ],
    },
    "uipath-llm-judge-output-strict-json-similarity": {
        "name": "LLM Judge Strict JSON Similarity Evaluator",
        "description": "Uses an LLM for strict JSON comparison between expected and actual output.",
        "category": "output",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "LLMJudgeStrictJSONSimilarityOutputEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "targetOutputKey",
                "label": "Target Output Key",
                "type": "string",
                "default": "*",
                "required": False,
            },
            {
                "name": "model",
                "label": "Model",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "prompt",
                "label": "Prompt",
                "type": "textarea",
                "default": "Compare the following JSON outputs for strict structural similarity.\n\nActual Output: {{ActualOutput}}\nExpected Output: {{ExpectedOutput}}\n\nEvaluate if the JSON structure and values match precisely. Provide a score from 0-100 where 100 means exact match and 0 means completely different.",
                "required": False,
            },
            {
                "name": "temperature",
                "label": "Temperature",
                "type": "number",
                "default": 0.0,
                "required": False,
            },
            {
                "name": "maxTokens",
                "label": "Max Tokens",
                "type": "integer",
                "default": None,
                "required": False,
            },
        ],
        "criteria_fields": [
            {
                "name": "expectedOutput",
                "label": "Expected Output",
                "type": "json",
                "required": True,
            },
        ],
    },
    "uipath-tool-call-order": {
        "name": "Tool Call Order Evaluator",
        "description": "Evaluates whether tools were called in the expected order.",
        "category": "trajectory",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "ToolCallOrderEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "strict",
                "label": "Strict",
                "type": "boolean",
                "default": False,
                "required": False,
                "description": "If true, requires exact order match; otherwise allows subsequence",
            },
        ],
        "criteria_fields": [
            {
                "name": "toolCallsOrder",
                "label": "Tool Calls Order",
                "type": "string_array",
                "required": True,
                "description": "List of tool names in expected order",
            },
        ],
    },
    "uipath-tool-call-count": {
        "name": "Tool Call Count Evaluator",
        "description": "Evaluates the count of specific tool calls.",
        "category": "trajectory",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "ToolCallCountEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "strict",
                "label": "Strict",
                "type": "boolean",
                "default": False,
                "required": False,
                "description": "If true, requires exact count match",
            },
        ],
        "criteria_fields": [
            {
                "name": "toolCallsCount",
                "label": "Tool Calls Count",
                "type": "json",
                "required": True,
                "description": "Object mapping tool names to [comparison, count] tuples",
            },
        ],
    },
    "uipath-tool-call-args": {
        "name": "Tool Call Args Evaluator",
        "description": "Evaluates the arguments passed to specific tool calls.",
        "category": "trajectory",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "ToolCallArgsEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "strict",
                "label": "Strict",
                "type": "boolean",
                "default": False,
                "required": False,
            },
            {
                "name": "subset",
                "label": "Subset",
                "type": "boolean",
                "default": False,
                "required": False,
                "description": "If true, only checks that expected args are present (not exact match)",
            },
        ],
        "criteria_fields": [
            {
                "name": "toolCalls",
                "label": "Tool Calls",
                "type": "json",
                "required": True,
                "description": "Array of {name, args} objects specifying expected tool calls",
            },
        ],
    },
    "uipath-tool-call-output": {
        "name": "Tool Call Output Evaluator",
        "description": "Evaluates the outputs from specific tool calls.",
        "category": "trajectory",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "ToolCallOutputEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "strict",
                "label": "Strict",
                "type": "boolean",
                "default": False,
                "required": False,
            },
        ],
        "criteria_fields": [
            {
                "name": "toolOutputs",
                "label": "Tool Outputs",
                "type": "json",
                "required": True,
                "description": "Array of {name, output} objects specifying expected tool outputs",
            },
        ],
    },
    "uipath-llm-judge-trajectory-similarity": {
        "name": "LLM Judge Trajectory Evaluator",
        "description": "Uses an LLM to evaluate the agent's execution trajectory.",
        "category": "trajectory",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "LLMJudgeTrajectoryEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "model",
                "label": "Model",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "prompt",
                "label": "Prompt",
                "type": "textarea",
                "default": "As an expert evaluator, determine how well the agent performed on a scale of 0-100. Focus on whether the agent's actions and outputs matched the expected behavior, while allowing for alternative valid expressions and reasonable variations in language. Maintain high standards for accuracy and completeness. Provide your score with a brief and clear justification explaining your reasoning.\n----\nAgentInput:\n{{UserOrSyntheticInput}}\n----\nExpectedAgentBehavior:\n{{ExpectedAgentBehavior}}\n----\nAgentRunHistory:\n{{AgentRunHistory}}\n",
                "required": False,
            },
            {
                "name": "temperature",
                "label": "Temperature",
                "type": "number",
                "default": 0.0,
                "required": False,
            },
            {
                "name": "maxTokens",
                "label": "Max Tokens",
                "type": "integer",
                "default": None,
                "required": False,
            },
        ],
        "criteria_fields": [
            {
                "name": "expectedAgentBehavior",
                "label": "Expected Agent Behavior",
                "type": "textarea",
                "required": True,
                "description": "Description of expected agent behavior",
            },
        ],
    },
    "uipath-llm-judge-trajectory-simulation": {
        "name": "LLM Judge Trajectory Simulation Evaluator",
        "description": "Uses an LLM to evaluate agent trajectory with simulation instructions.",
        "category": "trajectory",
        "config_fields": [
            {
                "name": "name",
                "label": "Name",
                "type": "string",
                "default": "LLMJudgeTrajectorySimulationEvaluator",
                "required": False,
            },
            {
                "name": "description",
                "label": "Description",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "model",
                "label": "Model",
                "type": "string",
                "default": "",
                "required": False,
            },
            {
                "name": "prompt",
                "label": "Prompt",
                "type": "textarea",
                "default": "As an expert evaluator, determine how well the agent did on a scale of 0-100. Focus on if the simulation was successful and if the agent behaved according to the expected output accounting for alternative valid expressions, and reasonable variations in language while maintaining high standards for accuracy and completeness. Provide your score with a justification, explaining briefly and concisely why you gave that score.\n----\nAgentInput:\n{{UserOrSyntheticInput}}\n----\nSimulationInstructions:\n{{SimulationInstructions}}\n----\nExpectedAgentBehavior:\n{{ExpectedAgentBehavior}}\n----\nAgentRunHistory:\n{{AgentRunHistory}}\n",
                "required": False,
            },
            {
                "name": "temperature",
                "label": "Temperature",
                "type": "number",
                "default": 0.0,
                "required": False,
            },
            {
                "name": "maxTokens",
                "label": "Max Tokens",
                "type": "integer",
                "default": None,
                "required": False,
            },
        ],
        "criteria_fields": [
            {
                "name": "expectedAgentBehavior",
                "label": "Expected Agent Behavior",
                "type": "textarea",
                "required": True,
                "description": "Description of expected agent behavior",
            },
        ],
    },
}


def get_evaluator_type(type_id: str) -> dict[str, Any] | None:
    """Get evaluator type definition by ID."""
    return EVALUATOR_TYPES.get(type_id)
